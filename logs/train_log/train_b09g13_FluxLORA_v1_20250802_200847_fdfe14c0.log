# AutoTrainX Training Log
# Dataset: b09g13_FluxLORA_v1
# Preset: FluxLORA
# Job ID: fdfe14c0
# Start Time: 2025-08-02T20:08:47.534639
# Mode: variations
# Command: /home/eqx/AutoTrainX/venv/bin/python /home/eqx/AutoTrainX/sd-scripts/flux_train_network.py --config_file /home/eqx/AutoTrainX/workspace/Presets/Variations/FluxLORA/b09g13_FluxLORA_v1_fdfe14c0.toml
================================================================================

2025-08-02 20:08:50 INFO     Loading settings from            train_util.py:4651
                             /home/eqx/AutoTrainX/workspace/P                   
                             resets/Variations/FluxLORA/b09g1                   
                             3_FluxLORA_v1_fdfe14c0.toml...                     
                    INFO     highvram is enabled /            train_util.py:4316
                             highvramが有効です                                 
2025-08-02 20:08:50 INFO     Checking the state dict: Diffusers flux_utils.py:43
                             or BFL, dev or schnell                             
                    INFO     t5xxl_max_token_length:   flux_train_network.py:157
                             512                                                
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
2025-08-02 20:08:51 INFO     Using DreamBooth method.       train_network.py:517
                    INFO     prepare images.                  train_util.py:2072
                    INFO     get image size from name of      train_util.py:1965
                             cache files                                        

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 42799.02it/s]
                    INFO     set image size from cache files: train_util.py:1995
                             0/1                                                
                    INFO     found directory                  train_util.py:2019
                             /home/eqx/AutoTrainX/workspace/o                   
                             utput/b09g13/img/30_b09g13                         
                             person contains 1 image files                      

read caption:   0%|          | 0/1 [00:00<?, ?it/s]
read caption: 100%|██████████| 1/1 [00:00<00:00, 19878.22it/s]
                    INFO     30 train images with repeats.    train_util.py:2116
                    INFO     0 reg images with repeats.       train_util.py:2120
                    WARNING  no regularization images /       train_util.py:2125
                             正則化画像が見つかりませんでした                   
                    INFO     [Dataset 0]                      config_util.py:580
                               batch_size: 1                                    
                               resolution: (1024, 1280)                         
                               resize_interpolation: None                       
                               enable_bucket: False                             
                                                                                
                               [Subset 0 of Dataset 0]                          
                                 image_dir:                                     
                             "/home/eqx/AutoTrainX/workspace/                   
                             output/b09g13/img/30_b09g13                        
                             person"                                            
                                 image_count: 1                                 
                                 num_repeats: 30                                
                                 shuffle_caption: False                         
                                 keep_tokens: 0                                 
                                 caption_dropout_rate: 0.0                      
                                 caption_dropout_every_n_epoc                   
                             hs: 0                                              
                                 caption_tag_dropout_rate:                      
                             0.0                                                
                                 caption_prefix: None                           
                                 caption_suffix: None                           
                                 color_aug: False                               
                                 flip_aug: False                                
                                 face_crop_aug_range: None                      
                                 random_crop: False                             
                                 token_warmup_min: 1,                           
                                 token_warmup_step: 0,                          
                                 alpha_mask: False                              
                                 resize_interpolation: None                     
                                 custom_attributes: {}                          
                                 is_reg: False                                  
                                 class_tokens: b09g13 person                    
                                 caption_extension: .txt                        
                                                                                
                                                                                
                    INFO     [Prepare dataset 0]              config_util.py:592
                    INFO     loading image sizes.              train_util.py:987

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 22075.28it/s]
                    INFO     prepare dataset                  train_util.py:1012
                    INFO     preparing accelerator          train_network.py:580
                    INFO     Checking the state dict: Diffusers flux_utils.py:43
                             or BFL, dev or schnell                             
                    INFO     Building Flux model dev from BFL  flux_utils.py:101
                             checkpoint                                         
                    INFO     Loading state dict from           flux_utils.py:118
                             /home/eqx/AutoTrainX/models/flux1                  
                             -dev-fp8.safetensors                               
                    INFO     Loaded Flux: <All keys matched    flux_utils.py:137
                             successfully>                                      
                    INFO     Loaded fp8 FLUX model     flux_train_network.py:106
                    INFO     Building CLIP-L                   flux_utils.py:179
                    INFO     Loading state dict from           flux_utils.py:275
                             /home/eqx/AutoTrainX/models/clip_                  
                             l.safetensors                                      
                    INFO     Loaded CLIP-L: <All keys matched  flux_utils.py:278
                             successfully>                                      
                    INFO     Loading state dict from           flux_utils.py:330
                             /home/eqx/AutoTrainX/models/t5xxl                  
                             _fp8_e4m3fn.safetensors                            
                    INFO     Loaded T5xxl: <All keys matched   flux_utils.py:333
                             successfully>                                      
                    INFO     Loaded fp8 T5XXL model    flux_train_network.py:140
                    INFO     Building AutoEncoder              flux_utils.py:144
                    INFO     Loading state dict from           flux_utils.py:149
                             /home/eqx/AutoTrainX/models/ae.sa                  
                             fetensors                                          
                    INFO     Loaded AE: <All keys matched      flux_utils.py:152
                             successfully>                                      
                    INFO     [Dataset 0]                      train_util.py:2613
                    INFO     caching latents with caching     train_util.py:1115
                             strategy.                                          
                    INFO     caching latents...               train_util.py:1164
accelerator device: cuda
import network module: networks.lora_flux

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 1006.31it/s]
2025-08-02 20:08:52 INFO     move vae and unet to cpu  flux_train_network.py:210
                             to save memory                                     
                    INFO     move text encoders to gpu flux_train_network.py:218
2025-08-02 20:08:53 INFO     prepare T5XXL for fp8:    flux_train_network.py:511
                             set to                                             
                             torch.float8_e4m3fn, set                           
                             embeddings to                                      
                             torch.bfloat16, add hooks                          
                    INFO     [Dataset 0]                      train_util.py:2635
                    INFO     caching Text Encoder outputs     train_util.py:1298
                             with caching strategy.                             
                    INFO     checking cache validity...       train_util.py:1309

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 19878.22it/s]
                    INFO     caching Text Encoder outputs...  train_util.py:1340

  0%|          | 0/1 [00:00<?, ?it/s]                    WARNING  T5 model is using fp8 weights  strategy_flux.py:160
                             for caching. This may affect                       
                             the quality of the cached                          
                             outputs. /                                         
                             T5モデルはfp8の重みを使用して                      
                             います。これはキャッシュの品質                     
                             に影響を与える可能性があります                     
                             。                                                 

100%|██████████| 1/1 [00:00<00:00,  3.54it/s]
100%|██████████| 1/1 [00:00<00:00,  3.54it/s]
                    INFO     cache Text Encoder        flux_train_network.py:234
                             outputs for sample                                 
                             prompt:                                            
                             /home/eqx/AutoTrainX/work                          
                             space/output/b09g13/sampl                          
                             e_prompts.txt                                      
                    INFO     cache Text Encoder        flux_train_network.py:245
                             outputs for prompt:                                
                             b09g13, Photo of a young                           
                             woman with light skin,                             
                             blonde hair styled in a                            
                             side braid, wearing a                              
                             black fur coat, sitting                            
                             in a car, looking                                  
                             directly at the camera                             
                             with a neutral                                     
                             expression, parted lips,                           
                             and freckles on her                                
                             cheeks. The background is                          
                             dark, and the lighting is                          
                             dim, creating a moody                              
                             atmosphere. The woman is                           
                    INFO     cache Text Encoder        flux_train_network.py:245
                             outputs for prompt:                                
2025-08-02 20:08:54 INFO     move t5XXL back to cpu    flux_train_network.py:258
2025-08-02 20:08:56 INFO     move vae and unet back to flux_train_network.py:263
                             original device                                    
                    INFO     create LoRA network. base dim      lora_flux.py:743
                             (rank): 32, alpha: 16                              
                    INFO     neuron dropout: p=None, rank       lora_flux.py:744
                             dropout: p=None, module dropout:                   
                             p=None                                             
                    INFO     train all blocks only              lora_flux.py:758
                    INFO     create LoRA for Text Encoder 1:    lora_flux.py:898
                    INFO     create LoRA for Text Encoder 1: 72 lora_flux.py:901
                             modules.                                           
2025-08-02 20:08:58 INFO     create LoRA for FLUX all blocks:   lora_flux.py:922
                             304 modules.                                       
                    INFO     enable LoRA for text encoder: 72  lora_flux.py:1098
                             modules                                            
                    INFO     enable LoRA for U-Net: 304        lora_flux.py:1103
                             modules                                            
                    INFO     Text Encoder 1 (CLIP-L): 72       lora_flux.py:1205
                             modules, LR 5e-05                                  
                    INFO     use Adafactor optimizer |        train_util.py:4963
                             {'scale_parameter': False,                         
                             'relative_step': False,                            
                             'warmup_init': False,                              
                             'weight_decay': 0.01}                              
                    WARNING  because max_grad_norm is set,    train_util.py:4991
                             clip_grad_norm is enabled.                         
                             consider set to 0 /                                
                             max_grad_normが設定されているた                    
                             めclip_grad_normが有効になります                   
                             。0に設定して無効にしたほうがい                    
                             いかもしれません                                   
                    WARNING  constant_with_warmup will be     train_util.py:4995
                             good /                                             
                             スケジューラはconstant_with_warm                   
                             upが良いかもしれません                             
                    INFO     set U-Net weight dtype to      train_network.py:826
                             torch.float8_e4m3fn                                
                    INFO     prepare CLIP-L for fp8:   flux_train_network.py:482
                             set to                                             
                             torch.float8_e4m3fn, set                           
                             embeddings to                                      
                             torch.bfloat16                                     
fatal: not a git repository (or any of the parent directories): .git
2025-08-02 20:09:06 INFO     unet dtype:                   train_network.py:1323
                             torch.float8_e4m3fn, device:                       
                             cuda:0                                             
                    INFO     text_encoder [0] dtype:       train_network.py:1329
                             torch.float8_e4m3fn, device:                       
                             cuda:0                                             
                    INFO     text_encoder [1] dtype:       train_network.py:1329
                             torch.float8_e4m3fn, device:                       
                             cpu                                                
FLUX: Gradient checkpointing enabled. CPU offload: False
prepare optimizer, data loader etc.
override steps. steps for 1 epochs is / 指定エポックまでのステップ数: 30
enable full bf16 training.
enable fp8 training for U-Net.
enable fp8 training for Text Encoder.
running training / 学習開始
  num train images * repeats / 学習画像の数×繰り返し回数: 30
  num validation images * repeats / 学習画像の数×繰り返し回数: 0
  num reg images / 正則化画像の数: 0
  num batches per epoch / 1epochのバッチ数: 30
  num epochs / epoch数: 1
  batch size per device / バッチサイズ: 1
  gradient accumulation steps / 勾配を合計するステップ数 = 1
  total optimization steps / 学習ステップ数: 30

steps:   0%|          | 0/30 [00:00<?, ?it/s]                    INFO     epoch is incremented.             train_util.py:779
                             current_epoch: 0, epoch: 1                         

steps:   3%|▎         | 1/30 [00:03<01:32,  3.20s/it]
steps:   3%|▎         | 1/30 [00:03<01:32,  3.20s/it, avr_loss=0.617]
steps:   7%|▋         | 2/30 [00:06<01:25,  3.05s/it, avr_loss=0.617]
steps:   7%|▋         | 2/30 [00:06<01:25,  3.05s/it, avr_loss=0.67] 
steps:  10%|█         | 3/30 [00:09<01:21,  3.03s/it, avr_loss=0.67]
steps:  10%|█         | 3/30 [00:09<01:21,  3.03s/it, avr_loss=0.551]
steps:  13%|█▎        | 4/30 [00:12<01:18,  3.01s/it, avr_loss=0.551]
steps:  13%|█▎        | 4/30 [00:12<01:18,  3.01s/it, avr_loss=0.552]
steps:  17%|█▋        | 5/30 [00:14<01:14,  2.99s/it, avr_loss=0.552]
steps:  17%|█▋        | 5/30 [00:14<01:14,  2.99s/it, avr_loss=0.565]
steps:  20%|██        | 6/30 [00:17<01:11,  2.99s/it, avr_loss=0.565]
steps:  20%|██        | 6/30 [00:17<01:11,  2.99s/it, avr_loss=0.554]
steps:  23%|██▎       | 7/30 [00:20<01:08,  2.98s/it, avr_loss=0.554]
steps:  23%|██▎       | 7/30 [00:20<01:08,  2.98s/it, avr_loss=0.566]
steps:  27%|██▋       | 8/30 [00:23<01:05,  2.97s/it, avr_loss=0.566]
steps:  27%|██▋       | 8/30 [00:23<01:05,  2.97s/it, avr_loss=0.576]
steps:  30%|███       | 9/30 [00:26<01:02,  2.96s/it, avr_loss=0.576]
steps:  30%|███       | 9/30 [00:26<01:02,  2.96s/it, avr_loss=0.55] 
steps:  33%|███▎      | 10/30 [00:29<00:58,  2.94s/it, avr_loss=0.55]
steps:  33%|███▎      | 10/30 [00:29<00:58,  2.94s/it, avr_loss=0.556]
steps:  37%|███▋      | 11/30 [00:32<00:55,  2.94s/it, avr_loss=0.556]
steps:  37%|███▋      | 11/30 [00:32<00:55,  2.94s/it, avr_loss=0.571]
steps:  40%|████      | 12/30 [00:35<00:52,  2.93s/it, avr_loss=0.571]
steps:  40%|████      | 12/30 [00:35<00:52,  2.93s/it, avr_loss=0.586]
steps:  43%|████▎     | 13/30 [00:38<00:49,  2.94s/it, avr_loss=0.586]
